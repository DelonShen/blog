---
title: Introduction to Algorithms(CLRS) Notes
---
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
  <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
  <meta http-equiv="Content-Style-Type" content="text/css" />
  <meta name="generator" content="pandoc" />
  <meta name="author" content="Delon Shen" />
  <title></title>
  <style type="text/css">code{white-space: pre;}</style>
  <script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS_CHTML-full" type="text/javascript"></script>
</head>
<body>
<div id="TOC">
<ul>
<li><a href="#chapter-1-role-of-algorithms-in-computing">Chapter 1: Role of Algorithms in Computing</a><ul>
<li><a href="#algorithms">Algorithms</a><ul>
<li><a href="#exercises">Exercises</a></li>
</ul></li>
<li><a href="#algorithms-as-a-technology">Algorithms as a technology</a><ul>
<li><a href="#exercises-1">Exercises</a></li>
</ul></li>
<li><a href="#problems">Problems</a></li>
</ul></li>
<li><a href="#chapter-2-getting-started">Chapter 2: Getting Started</a><ul>
<li><a href="#insertion-sort">Insertion Sort</a><ul>
<li><a href="#insertion-sort-in-c">Insertion Sort in C++</a></li>
<li><a href="#exercises-2">Exercises</a></li>
</ul></li>
</ul></li>
<li><a href="#chapter-30-polynomials-and-the-fft">Chapter 30: Polynomials and the FFT</a><ul>
<li><a href="#representing-polynomials">Representing polynomials</a></li>
<li><a href="#the-dft-and-fft">The DFT and FFT</a><ul>
<li><a href="#roots-of-unity">Roots of Unity</a></li>
<li><a href="#inverse-fast-fourier-transform">Inverse Fast Fourier Transform</a></li>
</ul></li>
</ul></li>
<li><a href="#revision-history">Revision History</a></li>
</ul>
</div>
<p><em>Note: I was originally planning on going cover to cover but decided that skipping around would be more fun.</em></p>
<h1 id="chapter-1-role-of-algorithms-in-computing">Chapter 1: Role of Algorithms in Computing</h1>
<h2 id="algorithms">Algorithms</h2>
<p>An <strong>algorithm</strong> is something that transforms an input into an output through a sequence of computational steps that solves a <strong>computational problem</strong>. An example of a computational problem is the sorting problem. An <strong>instance</strong> of the sorting problem is to sort the sequence (3,5,74,2,1,23,5). A algorithm is defined as correct if the algorithm <em>always halts on the correct output and doesn’t halt on any inputs.</em><br />
Problems Solved by Algorithms</p>
<ul>
<li><p>Sequencing genomes</p></li>
<li><p>Information retrieval (internet)</p></li>
<li><p>Securing personal information (cryptography)</p></li>
<li><p>Allocating resources (linear programming)</p></li>
<li><p>Shortest path</p></li>
<li><p>Longest common sub-sequence</p></li>
<li><p>Topological sorting</p></li>
<li><p>Finding smallest convex polygon containing points</p></li>
</ul>
<p>A <strong>data structure</strong> is way to organize data to help with retrieval and modification.<br />
<strong>NP-complete</strong> problems are problems with no known efficient solutions. They’re interesting since there is no proof that an efficient algorithm doesn’t exisist for NP-complete problems, if an efficient algorithm is found for one NP-complete problem then there is an efficient algorithm for all NP-complete problems, and many NP-complete problems are problems that we know efficient algorithms for but are slightly modified. An example of a NP-complete problem is the <a href="https://simple.wikipedia.org/wiki/Travelling_salesman_problem"><strong>Travelling Salesman Problem.</strong></a></p>
<h3 id="exercises">Exercises</h3>
<p><strong>1.1-1.</strong> In self driving cars, computing the convex hull of other cars may help with collision avoidance<br />
<strong>1.1-2.</strong> Memory<br />
<strong>1.1-3.</strong> <a href="https://en.wikipedia.org/wiki/Bogosort">Bogosort</a> is very good at always sorting sequences but it is very slow.<br />
<strong>1.1-4.</strong> Both rely try to find the shortest path but traveling-salesman has a lot more stops to make.<br />
<strong>1.1-5.</strong> An algorithm to divide a number by 3 needs best solution. An algorithm to determine risk of diabetes can be approximate.</p>
<h2 id="algorithms-as-a-technology">Algorithms as a technology</h2>
<p>Even if our computers were infinitely fast and memory was infinite and free somehow, we’d still want to study algorithms to understand whether they were correct or not.<br />
<br />
Insertion sort takes time roughly <span class="math inline">\(c_1n^2\)</span> while merge sort takes time roughly <span class="math inline">\(c_2nlog_2(n)\)</span>. Lets compare the two algorithms by running insertion sort on computer A which does <span class="math inline">\(10^{10}\)</span> instructions per second while running merge sort on computer B which does <span class="math inline">\(10^{7}\)</span> instructions per second on an input of the size <span class="math inline">\(10^7\)</span>. Also lets assume <span class="math inline">\(c_1\)</span> (the constant term in insertion sort’s running time) is 2 since Donald Knuth programmed insertion sort in <a href="https://en.wikipedia.org/wiki/MMIX">MMIX</a> and <span class="math inline">\(c_2\)</span> (constant term in merge sort’s running time) is 50 since my cat coded merge sort in <a href="https://scratch.mit.edu/">scratch</a>.<br />
<strong>Computer A Running Insertion Sort Analysis</strong> <span class="math display">\[\frac{2\cdot(10^7)^2\text{ instructions}}{10^{10}\text{ instructions/second}}=20,000\text{ seconds or about 5.56 hours}\]</span> <strong>Computer B Running Merge Sort Analysis</strong> <span class="math display">\[\frac{50\cdot(10^7)log_2(10^7)\text{ instructions}}{10^{7}\text{ instructions/second}}=1163 \text{ seconds or about 19.4 minutes}\]</span></p>
<p>It’s pretty clear that even though computer B is much slower than computer A (1000 times slower in fact) and merge sort was coded more inefficiently, it still won out in the end due to it’s behavior. <em>As the problem size increases, merge sort’s advantage over insertion sort is magnified.</em></p>
<h3 id="exercises-1">Exercises</h3>
<p><strong>1.2-1.</strong> Google maps needs to find the shortest path.<br />
<strong>1.2-2.</strong> <span class="math inline">\(8n^2&lt;64nlgn\Rightarrow\)</span> By wolfram alpha <span class="math inline">\( n \approx 43\)</span><br />
<strong>1.2-3.</strong> <span class="math inline">\(100n^2&lt;2^n\Rightarrow\)</span> By wolfram alpha <span class="math inline">\(n=15\)</span></p>
<h2 id="problems">Problems</h2>
<p><strong>1-1.</strong> Just solve for n in <span class="math inline">\(f(n)= t\text{ seconds}* 10^{6}\)</span> to fill in chart. e.g. <span class="math display">\[n^2=60 \text{ seconds} * 10^6\]</span> <span class="math display">\[n=7745.966\]</span></p>
<h1 id="chapter-2-getting-started">Chapter 2: Getting Started</h1>
<h2 id="insertion-sort">Insertion Sort</h2>
<p><strong>Insertion sort</strong> works similar to how most people sort a hand of playing cards. We start by looking at the second card and seeing if it’s greater than or less than the first card. If it’s greater than, we leave it where it is. If it’s less than, we insert if before the first card. Afterwards we look at the third card and insert it properly into the sequence of the first two cards so that now the first three cards are properly sorted. This process continues for all the cards.</p>
<p><em>Example: sort the sequence (6,9,3,5)</em><br />
6 9 3 5<br />
3 6 9 5<br />
3 5 6 9</p>
<h3 id="insertion-sort-in-c">Insertion Sort in C++</h3>
<pre><code>void insertion_sort(vector&lt;int&gt;&amp; A){
  for(int x = 1; x&lt;A.size(); x++){
    int curr = A[x]; //1
    int i = x-1;
    while(i&gt;=0 &amp;&amp; A[i] &gt; curr){ //2
      A[i+1] = A[i];
      i--;
    }
    A[i+1] = curr; //3
  }
}</code></pre>
<p>For each elemnt, we assign the variable <em>curr</em> to it (1). Then we start looking at the previous sequence and keep shifting elements until we find the proper place (2) to insert the current element we’re trying to insert. After this place is found, we insert the element there and move onto the next element.<br />
<br />
At every iteration, the subarry from index 0 to x-1 is sorted. We’ll call this property a <strong>loop invariant</strong> and use this property to understand why the algorithm works. We must show that the loop invariant is</p>
<ol>
<li><p><strong>Initialization:</strong> Loop invariant is true before the first loop begins</p></li>
<li><p><strong>Maintenance:</strong> That if the loop invariant is true before one iteration, it must remain true for the next iteration.</p></li>
<li><p><strong>Termination:</strong> That when the loop terminates, the invariant helps show the algorithm works</p></li>
</ol>
<p>Applying this to insertion sort:</p>
<ol>
<li><p><strong>Initialization:</strong> We start by looking at the second elment which is at index 1. The subarray from index 0 to 0 is sorted since it is just one element. Therefore the loop invariant is true before the first loop begins.</p></li>
<li><p><strong>Maintenance:</strong> After the iteration is done for index y, the subarray from index 0 to y is sorted. Therefore for the next iteration, the subarray from index 0 to x-1 which in this case is (y+1)-1 is sorted thus preserving the loop invariant.</p></li>
<li><p><strong>Termination:</strong> The loop terminates when x=A.size() or when x is the array size which we’ll call n. This means that the subarray from index 0 to n-1 is sorted. Notice how this subarray is simply the entire orignal array. We can now conclude the entire array is sorted meaning the algorithm works.</p></li>
</ol>
<h3 id="exercises-2">Exercises</h3>
<p><strong>2.1-1.</strong></p>
<p>31 41 59 26 41 58<br />
31 41 59 26 41 58<br />
26 31 41 59 41 58<br />
26 31 41 41 59 58<br />
26 31 41 41 58 59</p>
<p><strong>2.1-2</strong></p>
<pre><code>void insertion_sort(vector&lt;int&gt;&amp; A){
  for(int x = 1; x&lt;A.size(); x++){
    int curr = A[x]; //1
    int i = x-1;
    while(i&gt;=0 &amp;&amp; A[i] &lt; curr){ //2
      A[i+1] = A[i];
      i--;
    }
    A[i+1] = curr; //3
  }
}</code></pre>
<p><strong>2.1-3.</strong></p>
<pre><code>int linear_search(vector&lt;int&gt;&amp; A, int e){
  for(int x = 0; x&lt;A.size(); x++){
    if(A[x]==e)
      return x;
  }
  return -1;
}</code></pre>
<p><strong>Loop invariant:</strong> For index x, there is no index y where y&lt;x where A[y]=e<br />
<strong>Initialization:</strong> Loop invariant is true since there are not index y where y&lt;x<br />
<strong>Maintenance:</strong> If loop invariant is true for iteration at index k and we move onto the next iteration, then A[k]!=e therefore the loop invariant holds for iteration at index k+1<br />
<strong>Termination:</strong> Loop terminates if either the element is found or the index is no longer less than the size of the array. In both cases the algorithm works since either we prove the element in not in the array or we found the element and returned the index.<br />
<strong>2.1-4.</strong><br />
<strong>Input:</strong> Two arrays A and B of size n containing two n-bit binary integeres<br />
<strong>Output:</strong> One array, C, of size n+1 containing the sum of the two n-bit binary integers stored in A and B</p>
<pre><code>vector&lt;int&gt; binary_add(vector&lt;int&gt;&amp; A, vector&lt;int&gt;&amp; B){
  int n = A.size();
  vector&lt;int&gt; C(n+1);
  int carry = 0;
  for(int x = n-1; x&gt;=0; x--){
    C[x+1] = A[x]+B[x]+carry;
    if(C[x+1]&gt;1){
      C[x+1]%=2;
      carry=1;
    }
    else
      carry=0;
  }
  C[0]=carry;
  return C;
}</code></pre>
<h1 id="chapter-30-polynomials-and-the-fft">Chapter 30: Polynomials and the FFT</h1>
<p>The fast Fourier transform reduced the complexity of multiplying together two polynomials from <span class="math inline">\(\Theta(n^2)\)</span> to <span class="math inline">\(\Theta(nlgn)\)</span>. <a href="https://www.youtube.com/watch?v=spUNpyF58BY">Neat introduction to fourier transform</a>.<br />
<strong>Polynomial</strong><span class="math display">\[A(x)=a_0+a_1x+a_2x^2+a_3x^3...a_{n-1}x^{n-1}=\sum_{j=0}^{n-1}a_jx^j=&lt;a_0, a_1,..,a_{n-1}&gt;\]</span> Things you might want to do with polynomials:</p>
<ul>
<li><p>Evaluate A(x) at <span class="math inline">\(x_0\)</span></p>
<ul>
<li><p>Naively you could compute x for every term but that would be quadratice time</p></li>
<li><p>Or you could use <strong>Horner’s Rule</strong>: <span class="math inline">\(A(x)=a_0+x(a_1+x(a_2+...x(a_{n-1})))\)</span> giving linear time</p></li>
</ul></li>
<li><p>Add polynomials: A(x)+B(x)=C(x)</p>
<ul>
<li><p><span class="math inline">\(c_k=a_k+b_k\)</span>, linear time</p></li>
</ul></li>
<li><p>Multiply polynomials A(x)B(x)=C(x)</p>
<ul>
<li><p><span class="math inline">\(c_k=\sum_{j=0}^ka_jb_{k-j}\)</span> which is O(<span class="math inline">\(n^2\)</span>)</p></li>
<li><p>This is why we like FFT</p></li>
</ul></li>
</ul>
<h2 id="representing-polynomials">Representing polynomials</h2>
<p><strong>Coefficient representation</strong> of <span class="math inline">\(A(x)=\sum_{j=0}^{n-1}a_jx^j\)</span> As we showed before you can evaluate this easily using Horner’s rule and adding is trivial. Multiplying however is rough, <span class="math inline">\(\Theta(n^2)\)</span>. The resulting coefficient vector is also called the <strong>convolution</strong> of vectors a and b, the two vectors we’re multiplying.<br />
<strong>Point-value representaiton</strong> of A(x) of degree-bound n is a set of n <strong>point-value pairs.</strong> <span class="math display">\[\{(x_0,y_0),(x_1,y_1),...(x_{n-1},y_{n-1})\}\]</span> where <span class="math inline">\(x_k\)</span> are distinct and <span class="math inline">\(y_k=A(x_k)\)</span>.<br />
So we want if we want to find the polynomical representation of this point-value pair we’d want it to satisfy <span class="math inline">\(y_k=A(x_k)\)</span>, in other words <span class="math display">\[a_0x_0+a_1x_0^2+a_3x_0^3+a_4x_0^4+...+a_{n-1}x_0^{n-1}=A(x_0)=y_0\]</span> <span class="math display">\[a_0x_1+a_1x_1^2+a_3x_1^3+a_4x_1^4+...+a_{n-1}x_1^{n-1}=A(x_1)=y_1\]</span> <span class="math display">\[a_0x_2+a_1x_2^2+a_3x_2^3+a_4x_2^4+...+a_{n-1}x_2^{n-1}=A(x_2)=y_2\]</span> <span class="math display">\[. . .\]</span> <span class="math display">\[a_0x_{n-1}+a_1x_{n-1}^2+a_3x_{n-1}^3+a_4x_{n-1}^4+...+a_{n-1}x_{n-1}^{n-1}=A(x_{n-1})=y_{n-1}\]</span> In this form it is pretty simple to see the matrix representation of this system of equations as <span class="math display">\[\begin{bmatrix}
  1 &amp; x_0 &amp; x_0^2 &amp; ... &amp; x_0^{n-1}\\1 &amp; x_1 &amp; x_1^2 &amp; ... &amp;x_1^{n-1}\\ \vdots &amp; \vdots &amp; \vdots &amp; \ddots &amp; \vdots \\ 1 &amp; x_{n-1} &amp; x_{n-1}^2 &amp; ... &amp; x_{n-1}^{n-1}

&amp;\end{bmatrix}
\begin{bmatrix}
  a_0 \\ a_1 \\ \vdots \\ a_{n-1}
\end{bmatrix}
=
\begin{bmatrix}
  y_0 \\ y_1 \\ \vdots \\ y_{n-1}
\end{bmatrix}\]</span> The left most matrix is known as the Vandermonde matrix, V, in which <span class="math inline">\(V_{jk} = x_j^k\)</span>. In this form we can see the transformation from coefficient representation to point-value representation as simply a matrix multiplication that will take <span class="math inline">\(O(n^2)\)</span>. In this form we can also see the transformation from point-value representation to coefficient representation as solving a system of linear equations. In math we would multiply by the inverse but since this is CS we’d have to use <a href="https://cp-algorithms.com/linear_algebra/linear-system-gauss.html"><strong>gaussian elimination!</strong></a> This is unfortunately <span class="math inline">\(O(n^3)\)</span> though but it does prove that there is a unique coefficient representation. <em>Note: there is a different proof in the book for the fact that there is a unique coefficient representation but this way made more sense to me. It is less rigorous though.</em><br />
In point-value representation, addition and multiplication is simply O(n). For example. <span class="math display">\[A = \{(x_0, y_0), (x_1, y_1), (x_2, y_2), ... , (x_{n-1}, y_{n-1})\}\]</span> <span class="math display">\[B = \{(x_0, y&#39;_0), (x_1, y&#39;_1), (x_2, y&#39;_2), ... , (x_{n-1}, y&#39;_{n-1})\}\]</span> <span class="math display">\[C = A+B = \{(x_0, y_0+y&#39;_0), (x_1, y_1+y&#39;_1), ... , (x_{n-1}, y_{n-1}+y&#39;_{n-1})\}\]</span> For multiplying polynomials we must take into account that the degree-bound of the resulting polynomial is the degree(A)+degree(B) where A and B are the two inputs. Therefore we must make sure that we have a “extended” point value represention of A and B with 2n point value pairs to make sure we have a unique polynomial of degree bound 2n. <span class="math display">\[A = \{(x_0, y_0), (x_1, y_1), (x_2, y_2), ... , (x_{2n-1}, y_{2n-1})\}\]</span> <span class="math display">\[B = \{(x_0, y&#39;_0), (x_1, y&#39;_1), (x_2, y&#39;_2), ... , (x_{2n-1}, y&#39;_{2n-1})\}\]</span> <span class="math display">\[C = A+B = \{(x_0, y_0+y&#39;_0), (x_1, y_1+y&#39;_1), ... , (x_{2n-1}, y_{2n-1}y&#39;_{2n-1})\}\]</span> The time is still <span class="math inline">\(O(n)\)</span> however.<br />
So the question now becomes how can we take advantage of this O(n) multiplication of two polynomials in point-value form to multiply two coefficient form polynomials. Our ability to do this efficiently hinges on being able to convert between the two forms efficiently and this is what the FFT gives us if we select our <span class="math inline">\(x_k\)</span>’s carefully.</p>
<h2 id="the-dft-and-fft">The DFT and FFT</h2>
<p><em>This section I mostly based of <a href="https://ocw.mit.edu/courses/electrical-engineering-and-computer-science/6-046j-design-and-analysis-of-algorithms-spring-2015/lecture-videos/lecture-3-divide-conquer-fft/">the 6.046 lecture on FFT</a></em><br />
So right now, converting from polynomial representation to point-value representation takes <span class="math inline">\(O(n^2)\)</span> time since it is matrix multiplication. However we can formulate a divide an conquer algorithm that manages do do this conversion in <span class="math inline">\(O(nlgn)\)</span> time if we select of <span class="math inline">\(x_k\)</span> carefully.<br />
<br />
So the goal of our algorithm is to compute A(x) for <span class="math inline">\(x\in X\)</span> where A(x) is a polynomial in coefficient representation and X is the set of the <span class="math inline">\(x_k\)</span> that we have chosen. We do this by</p>
<ol>
<li><p><strong>Divide:</strong> split the polynomial into even and odd coeficients <span class="math display">\[A_{even}(x)=\sum_{k=0}^{\lceil{\frac n 2 - 1}\rceil}a_{2k}x^k\]</span> <span class="math display">\[A_{odd}(x)=\sum_{k=0}^{\lceil{\frac n 2 - 1}\rceil}a_{2k+1}x^k\]</span></p></li>
<li><p><strong>Conquer:</strong> Recursively compute <span class="math inline">\(A_{even}(y)\)</span> and <span class="math inline">\(A_{odd}(y)\)</span> for all <span class="math inline">\(y\in X^2, X^2=\{x^2|x\in X\}\)</span>. The reason for using the set <span class="math inline">\(X^2\)</span> will become obvious in the combine step but it just comes down to algebra.</p></li>
<li><p><strong>Combine:</strong> Combine the terms <span class="math inline">\(A(x)=A_{even}(x^2)+xA_{odd}(x^2)\)</span> for <span class="math inline">\(x\in X\)</span>. In this step we use <span class="math inline">\(A_{(even/odd)}(x^2)\)</span> instead of <span class="math inline">\(A_{(even/odd)}(x)\)</span> since if you look at the original definition of those two summations, the x term is simply <span class="math inline">\(x^k\)</span> which mismatches with the coefficient term. This mismatch is also why we multiply <span class="math inline">\(A_{odd}\)</span> with x. Using <span class="math inline">\(A_{even/odd}(x^2)\)</span> is the reason we use the set of <span class="math inline">\(X^2\)</span> instead of <span class="math inline">\(X\)</span> in the conquer step.</p></li>
</ol>
<p>So lets try running this algorithm through a concrete example since I was a bit confused by it at first and I guess it would be useful. <span class="math display">\[A(x)=3x^2+9x^3+4x^5, X=\{2,3\}\]</span> <span class="math display">\[A_{even}=0+3x \rightarrow \text{Sub-problem 1}\]</span> <span class="math display">\[A_{odd}=0+9x+4x^2 \rightarrow \text{Sub-problem 2}\]</span></p>
<p><strong>Sub-Problem 1</strong> <span class="math display">\[A_1=3x, X=\{4,9\}\]</span> <span class="math display">\[A_{1,even}=0\]</span> <span class="math display">\[A_{1,odd}=3\]</span> <span class="math display">\[\Rightarrow A_1=0+3x=\{12, 27\}\]</span> <strong>Sub-Problem 2</strong> <span class="math display">\[A_2=9x+4x^2,X=\{4,9\}\]</span> <span class="math display">\[A_{2,even}=0+4x\rightarrow\text{Sub-problem 3}\]</span> <span class="math display">\[A_{2,odd}=9\]</span> <span class="math display">\[\Rightarrow A_2=A_{2,even}+xA_{2,odd}\]</span> <strong>Sub-problem 3</strong> <span class="math display">\[A_3=4x,X=\{16,81\}\]</span> <span class="math display">\[A_{3,even}=0\]</span> <span class="math display">\[A_{3,odd}=4\]</span> <span class="math display">\[\Rightarrow A_3=0+4x=\{64, 324\}\]</span> <span class="math display">\[\Rightarrow A_2 = \{64+9*4,\text{ } 324+9*9\}=\{100, 405\}\]</span> <span class="math display">\[\Rightarrow A=\{12+2*100,\text{ } 27+3*405 \}=\{212, 1242\}\]</span> <span class="math display">\[A(2)=212\]</span> <span class="math display">\[A(3)=1242\]</span></p>
<p>Hopefully that was helpful, it makes a lot more sense to me now at least. Anyway, lets try analyzing its efficiency. The recurrence relation of this algorithm can be written as <span class="math display">\[T(n, |X|)= 2\cdot T(\frac n 2, |X|)+O(n+ |X|)\]</span> At each step, the degree bound of sub-problems is halved but the size of the set <span class="math inline">\(X\)</span> remains the same. The <span class="math inline">\(O(n+|X|)\)</span> term comes from the partition(divide) step which is <span class="math inline">\(O(n)\)</span> and the conquer step which depends on the size of X which means it is <span class="math inline">\(O(|X|)\)</span> thus giving <span class="math inline">\(O(n+|X|)\)</span>. Also we know that at the root, <span class="math inline">\(n=|X|\)</span> if we want a valid point-value representation. This means <span class="math display">\[T(n, |X|)=2\cdot T(\frac n 2, |X|)+O(|X|)\]</span> It’s pretty obvious from this equation that this is not a very efficient algorithm since at every level there will be a cost of <span class="math inline">\(2^i\cdot O(n)\)</span> where i the level number for <span class="math inline">\(lgn\)</span> levels. In other words <span class="math display">\[\begin{aligned}
  T(n,|X|)&amp;=\sum_{i=0}^{lgn} n\cdot2^i\\
  &amp;=n\sum_{u=1}^n u,\text{ } u=2^i\\
  &amp;=n\cdot O(n^2)\\
  &amp;=O(n^3)\end{aligned}\]</span> <em>Note: Erik Demaine says it’s <span class="math inline">\(O(n^2)\)</span> and he’s probably definitely right but this is the answer I got and I’m not sure where I went wrong so I’ll just leave it here for now. What matters is that the algorithm is not great right now.</em><br />
<br />
The algorithm isn’t great and it’s because this <span class="math inline">\(|X|\)</span> term remains the same. We should notice that if <span class="math inline">\(|X|\)</span> <strong>collapsed</strong> for each subproblem, that is <span class="math inline">\(|X^2|=\frac {|X|} 2\)</span>, or <span class="math inline">\(|X|\)</span> was halved along with n and at each division step, then the recurrence relation would simply be <span class="math display">\[T(n)=2T(\frac n 2)+O(n)\]</span> It is easy to see that this means <span class="math inline">\(T(n)=O(nlgn)\)</span>. We can achieve this feature of “collapsing” by choosing our <span class="math inline">\(x_k\)</span> values very carefully by utilizing <strong>Roots of Unity</strong>.</p>
<h3 id="roots-of-unity">Roots of Unity</h3>
<p>We can construct a collapsing set with the power of square roots. For example, the <span class="math inline">\(X^2\)</span> is collapsing if <span class="math inline">\(X=\{1, -1\}\)</span>. And this set in turn is the <span class="math inline">\(X^2\)</span> set of <span class="math inline">\(\{1,-1,i,-i\}\)</span>. All of these points are spaced out evenly on the unit circle in the complex plane. We can find larger sets with this collapsing property by utilizing the equation <span class="math display">\[cos(\theta)+isin(\theta)=e^{i\theta}\]</span> with <span class="math display">\[\theta=2k\pi/n, k=0,1,2,...,n-1\]</span> From the equation <span class="math inline">\(e^{i\theta}\)</span> we can see that if we square a number we simply double the angle on the unit cirlce since <span class="math inline">\((e^{i\theta})^2=e^{i2\theta}\)</span>. However what if we double an angle if it’s bigger than <span class="math inline">\(\pi\)</span>? We would have gone around the entire circle and passed <span class="math inline">\(\theta=0\)</span>. In otherwords <span class="math inline">\(2\cdot \frac{3\pi}{2}=3\pi=\pi\)</span>. This lets us see that <span class="math inline">\((e^{i\theta})^2\)</span> actually equals <span class="math inline">\(e^{i2\theta\text{ mod }2\pi}\)</span> which gives the set of roots of unity the collapsing feature. For example, in the set <span class="math inline">\(X=\{1,-1,i,-i\}\)</span> we can figure out the angles to be <span class="math inline">\(\{0,\pi,\pi/2,3\pi/2\}\)</span> from Euler’s formula. But once we double the angles(aka finding the set <span class="math inline">\(X^2\)</span>) we get the set of angles of <span class="math inline">\(\{0, 2\pi, \pi, 3\pi\}=\{0,0,\pi,\pi\}\)</span>.<br />
<br />
From all this we can see that setting <span class="math inline">\(x_k=e^{i2\pi k/n}\)</span> gives us a collapsing <span class="math inline">\(X\)</span> set thus giving <span class="math inline">\(O(nlgn)\)</span> conversion from coefficient representation to point-value representation. <em>The Fast Fourier Transform is simply the divide and conquer algorithm we previously described with this special set of <span class="math inline">\(x_k\)</span> that we have chosen</em> Now all we have to do is figure out how to convert from point-value representation to coefficient representation efficiently and we’ll have a fast polynomial multiplication algorithm. But before we find the inverse transform lets summarize and introduce some notation to make our life easier. Fast polynomial multiplication can be summarized as<br />
We want to multiply two polynomials, A and B store the result in C. <span class="math display">\[A^*=FFT(A)\]</span> <span class="math display">\[B^*=FFT(B)\]</span> <span class="math display">\[C^*_k=A^*_kB^*_k\forall k\]</span> <span class="math display">\[C=IFFT(C^*)\]</span> Where IFFT is the inverse fast fourier transform. As we will see, this is very similar to the FFT.<br />
<br />
</p>
<h3 id="inverse-fast-fourier-transform">Inverse Fast Fourier Transform</h3>
<p>So converting from coefficient to point-value representation is simply the matrix product of the V, the Vandermonde matrix, and a, the coefficient matrix which we simplified with the FFT. The inverse FFT then is simply multiplying by <span class="math inline">\(V^{-1}\)</span>. This is pretty simple to see: <span class="math display">\[V^{-1}(V A)=V^{-1}A^*\]</span> <span class="math display">\[A=V^{-1}A\]</span> Luckily, <span class="math inline">\(V^{-1}\)</span> has a special form: <span class="math display">\[V^{-1}=\bar{V}/n\]</span> where <span class="math inline">\(\bar{V}\)</span> is the complex conjugate of V. Recall that <span class="math inline">\(V_{jk}=x_j^k=e^{ijk2\pi/n}\)</span> and the complex conjugate of <span class="math inline">\(e^{ijk2\pi/n}\)</span> is <span class="math inline">\(e^{-ijk2\pi/n}\)</span>. Thus we have the equation for the Inverse FFT: <span class="math display">\[(nV^{-1})A^*=nA\]</span> Now all there is to do is prove that neat formula for <span class="math inline">\(V^{-1}\)</span> and we’ll have a <span class="math inline">\(O(nlgn)\)</span> algorithm for polynomial multiplication.<br />
<br />
<strong>Prove that <span class="math inline">\(V^{-1}=\bar{V}/n\)</span></strong><br />
To do this we will prove that <span class="math inline">\(V\bar{V}=nI\)</span> where I is the identity matrix.</p>
<h1 id="revision-history">Revision History</h1>
<p>06-30-2019: Initial Commit<br />
07-01-2019: Chapter 1 Added<br />
07-02-2019: Chapter 2.1 Added<br />
07-03-2019: part of Chapter 30.1 added<br />
07-04-2019: part of Chapter 30.2 added<br />
</p>
<p><br />
<br />
<em>Think you found a mistake? You probably did. Let me know at <a href="mailto:hi@delonshen.com">hi@delonshen.com</a> if you want.</em></p>
</body>
</html>
